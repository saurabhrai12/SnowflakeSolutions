name: 🗄️ Snowflake Branch-Based Deployment Pipeline
# 
# Branch-based deployment strategy:
# - feature/*, feat/*, dev/* branches → DEV environment
# - main branch → STAGING environment  
# - Manual workflow_dispatch → Any environment (DEV/STAGING/PROD)
# - Pull requests → DEV environment (dry run)

on:
  # Manual trigger with environment selection
  workflow_dispatch:
    inputs:
      target_environment:
        description: 'Target Environment'
        required: true
        default: 'DEV'
        type: choice
        options:
          - DEV
          - STAGING
          - PROD
      deployment_mode:
        description: 'Deployment Mode'
        required: true
        default: 'INCREMENTAL'
        type: choice
        options:
          - INCREMENTAL
          - FULL
          - SCHEMA_SPECIFIC
      target_schemas:
        description: 'Target Schemas (comma-separated, leave empty for all)'
        required: false
        default: ''
        type: string
      dry_run:
        description: 'Dry Run (no actual deployment)'
        required: true
        default: true
        type: boolean
      load_sample_data:
        description: 'Load Sample Data (02_sample_data.sql)'
        required: false
        default: false
        type: boolean
      
  # Automatic triggers: feature branches → DEV, main → STAGING
  push:
    branches: [ main, 'feature/**', 'feat/**', 'dev/**' ]
    paths: [ 'sql/**' ]
    
  # Pull requests always deploy to DEV (dry run)
  pull_request:
    branches: [ main ]
    paths: [ 'sql/**' ]

env:
  # Build information
  BUILD_NUMBER: ${{ github.run_number }}
  GIT_COMMIT_SHORT: ${{ github.sha }}
  DEPLOYMENT_TIMESTAMP: ${{ github.run_id }}

jobs:
  # Job 1: Determine deployment parameters
  setup:
    name: 🏗️ Setup Deployment
    runs-on: ubuntu-latest
    outputs:
      target_environment: ${{ steps.setup.outputs.target_environment }}
      deployment_mode: ${{ steps.setup.outputs.deployment_mode }}
      target_schemas: ${{ steps.setup.outputs.target_schemas }}
      dry_run: ${{ steps.setup.outputs.dry_run }}
      load_sample_data: ${{ steps.setup.outputs.load_sample_data }}
      snowflake_database: ${{ steps.setup.outputs.snowflake_database }}
      snowflake_warehouse: ${{ steps.setup.outputs.snowflake_warehouse }}
      
    steps:
      - name: 📋 Setup Deployment Parameters
        id: setup
        run: |
          # Determine target environment
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            TARGET_ENV="${{ github.event.inputs.target_environment }}"
            DEPLOYMENT_MODE="${{ github.event.inputs.deployment_mode }}"
            TARGET_SCHEMAS="${{ github.event.inputs.target_schemas }}"
            DRY_RUN="${{ github.event.inputs.dry_run }}"
            LOAD_SAMPLE_DATA="${{ github.event.inputs.load_sample_data }}"
          elif [ "${{ github.event_name }}" = "push" ]; then
            # Branch-based environment selection
            BRANCH_NAME="${{ github.ref_name }}"
            if [ "$BRANCH_NAME" = "main" ]; then
              TARGET_ENV="STAGING"
              echo "🎯 Main branch detected - deploying to STAGING"
            elif [[ "$BRANCH_NAME" == feature/* ]] || [[ "$BRANCH_NAME" == feat/* ]] || [[ "$BRANCH_NAME" == dev/* ]]; then
              TARGET_ENV="DEV"
              echo "🎯 Feature branch ($BRANCH_NAME) detected - deploying to DEV"
            else
              TARGET_ENV="DEV"
              echo "🎯 Unknown branch ($BRANCH_NAME) - defaulting to DEV"
            fi
            DEPLOYMENT_MODE="INCREMENTAL"
            TARGET_SCHEMAS=""
            DRY_RUN="false"
            LOAD_SAMPLE_DATA="false"
          elif [ "${{ github.event_name }}" = "pull_request" ]; then
            TARGET_ENV="DEV"
            DEPLOYMENT_MODE="INCREMENTAL"
            TARGET_SCHEMAS=""
            DRY_RUN="true"
            LOAD_SAMPLE_DATA="false"
          fi
          
          # Set environment-specific database and warehouse
          case $TARGET_ENV in
            "DEV")
              SNOWFLAKE_DATABASE="analytics_platform_dev"
              SNOWFLAKE_WAREHOUSE="analytics_wh_dev"
              ;;
            "STAGING")
              SNOWFLAKE_DATABASE="analytics_platform_staging"
              SNOWFLAKE_WAREHOUSE="analytics_wh_staging"
              ;;
            "PROD")
              SNOWFLAKE_DATABASE="analytics_platform"
              SNOWFLAKE_WAREHOUSE="analytics_wh"
              ;;
          esac
          
          # Output parameters
          echo "target_environment=$TARGET_ENV" >> $GITHUB_OUTPUT
          echo "deployment_mode=$DEPLOYMENT_MODE" >> $GITHUB_OUTPUT
          echo "target_schemas=$TARGET_SCHEMAS" >> $GITHUB_OUTPUT
          echo "dry_run=$DRY_RUN" >> $GITHUB_OUTPUT
          echo "load_sample_data=$LOAD_SAMPLE_DATA" >> $GITHUB_OUTPUT
          echo "snowflake_database=$SNOWFLAKE_DATABASE" >> $GITHUB_OUTPUT
          echo "snowflake_warehouse=$SNOWFLAKE_WAREHOUSE" >> $GITHUB_OUTPUT
          
          # Display deployment plan
          echo "## 🗄️ Deployment Plan" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Environment | $TARGET_ENV |" >> $GITHUB_STEP_SUMMARY
          echo "| Database | $SNOWFLAKE_DATABASE |" >> $GITHUB_STEP_SUMMARY
          echo "| Warehouse | $SNOWFLAKE_WAREHOUSE |" >> $GITHUB_STEP_SUMMARY
          echo "| Mode | $DEPLOYMENT_MODE |" >> $GITHUB_STEP_SUMMARY
          echo "| Schemas | $TARGET_SCHEMAS |" >> $GITHUB_STEP_SUMMARY
          echo "| Dry Run | $DRY_RUN |" >> $GITHUB_STEP_SUMMARY
          echo "| Load Sample Data | $LOAD_SAMPLE_DATA |" >> $GITHUB_STEP_SUMMARY

  # Job 2: SQL Validation
  validate:
    name: 🔍 SQL Validation
    runs-on: ubuntu-latest
    needs: setup
    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4
        
      - name: 🔍 Validate SQL Schemas
        run: |
          echo "🔍 Validating SQL files for ${{ needs.setup.outputs.target_environment }}..."
          
          # Create validation report
          mkdir -p reports
          echo "# SQL Validation Report" > reports/sql-validation.md
          echo "Environment: ${{ needs.setup.outputs.target_environment }}" >> reports/sql-validation.md
          echo "" >> reports/sql-validation.md
          
          # Check for dangerous patterns
          DANGEROUS_PATTERNS=0
          if find sql/ -name "*.sql" -exec grep -l "DROP TABLE\|DROP DATABASE\|TRUNCATE" {} \; | wc -l | grep -v '^0$'; then
            echo "⚠️ Found dangerous DROP/TRUNCATE statements" >> reports/sql-validation.md
            DANGEROUS_PATTERNS=1
          fi
          
          # Check for idempotent patterns
          NON_IDEMPOTENT=0
          if find sql/ -name "*.sql" -exec grep -L "CREATE OR REPLACE\|CREATE.*IF NOT EXISTS" {} \; | wc -l | grep -v '^0$'; then
            echo "⚠️ Found non-idempotent CREATE statements" >> reports/sql-validation.md
            NON_IDEMPOTENT=1
          fi
          
          # Summary
          if [ $DANGEROUS_PATTERNS -eq 0 ] && [ $NON_IDEMPOTENT -eq 0 ]; then
            echo "✅ All SQL files passed validation" >> reports/sql-validation.md
            echo "✅ SQL validation passed"
          else
            echo "⚠️ SQL validation completed with warnings" >> reports/sql-validation.md
            echo "⚠️ SQL validation has warnings (non-blocking)"
          fi
          
      - name: 📊 Upload Validation Report
        uses: actions/upload-artifact@v4
        with:
          name: sql-validation-report
          path: reports/

  # Job 3: Deployment approval for STAGING/PROD
  approval:
    name: 🔒 Deployment Approval
    runs-on: ubuntu-latest
    needs: [setup, validate]
    if: contains(fromJSON('["STAGING", "PROD"]'), needs.setup.outputs.target_environment) && needs.setup.outputs.dry_run == 'false'
    environment: 
      name: ${{ needs.setup.outputs.target_environment }}
      
    steps:
      - name: 📋 Approval Required
        run: |
          echo "🔒 Manual approval required for ${{ needs.setup.outputs.target_environment }} deployment"
          echo "Database: ${{ needs.setup.outputs.snowflake_database }}"
          echo "Warehouse: ${{ needs.setup.outputs.snowflake_warehouse }}"
          echo "Mode: ${{ needs.setup.outputs.deployment_mode }}"

  # Job 4: SQL Schema Deployment
  deploy:
    name: 🗄️ Deploy to ${{ needs.setup.outputs.target_environment }}
    runs-on: ubuntu-latest
    needs: [setup, validate, approval]
    if: always() && needs.validate.result == 'success' && (needs.approval.result == 'success' || needs.approval.result == 'skipped')
    
    steps:
      - name: 📥 Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for git diff
        
      - name: 🔧 Install Dependencies
        run: |
          echo "📦 Installing Python dependencies..."
          pip install snowflake-connector-python
          
      - name: 🔌 Test Snowflake Connection
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets[format('SNOWFLAKE_{0}_ACCOUNT', needs.setup.outputs.target_environment)] }}
          SNOWFLAKE_USER: ${{ secrets[format('SNOWFLAKE_{0}_USER', needs.setup.outputs.target_environment)] }}
          SNOWFLAKE_PASSWORD: ${{ secrets[format('SNOWFLAKE_{0}_PASSWORD', needs.setup.outputs.target_environment)] }}
        run: |
          echo "🔌 Testing Snowflake connection..."
          python3 << 'EOF'
          import snowflake.connector
          import os
          
          try:
              conn = snowflake.connector.connect(
                  account=os.environ['SNOWFLAKE_ACCOUNT'],
                  user=os.environ['SNOWFLAKE_USER'],
                  password=os.environ['SNOWFLAKE_PASSWORD'],
                  warehouse='${{ needs.setup.outputs.snowflake_warehouse }}',
                  database='${{ needs.setup.outputs.snowflake_database }}'
              )
              
              cursor = conn.cursor()
              cursor.execute("SELECT CURRENT_USER(), CURRENT_ACCOUNT(), CURRENT_REGION()")
              result = cursor.fetchone()
              print(f"✅ Connected as: {result[0]} to {result[1]} in {result[2]}")
              
              cursor.close()
              conn.close()
              print("✅ Snowflake connection successful")
              
          except Exception as e:
              print(f"❌ Snowflake connection failed: {e}")
              exit(1)
          EOF
          
      - name: 🗄️ Deploy SQL Schemas
        if: needs.setup.outputs.dry_run == 'false'
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets[format('SNOWFLAKE_{0}_ACCOUNT', needs.setup.outputs.target_environment)] }}
          SNOWFLAKE_USER: ${{ secrets[format('SNOWFLAKE_{0}_USER', needs.setup.outputs.target_environment)] }}
          SNOWFLAKE_PASSWORD: ${{ secrets[format('SNOWFLAKE_{0}_PASSWORD', needs.setup.outputs.target_environment)] }}
        run: |
          echo "🚀 Deploying to ${{ needs.setup.outputs.target_environment }}..."
          mkdir -p deployment-logs
          
          python3 << 'EOF'
          import snowflake.connector
          import os
          from datetime import datetime
          
          # Connection parameters
          conn_params = {
              'account': os.environ['SNOWFLAKE_ACCOUNT'],
              'user': os.environ['SNOWFLAKE_USER'],
              'password': os.environ['SNOWFLAKE_PASSWORD']
          }
          
          database = '${{ needs.setup.outputs.snowflake_database }}'
          warehouse = '${{ needs.setup.outputs.snowflake_warehouse }}'
          deployment_mode = '${{ needs.setup.outputs.deployment_mode }}'
          
          try:
              print("🔗 Connecting to Snowflake...")
              conn = snowflake.connector.connect(**conn_params)
              cursor = conn.cursor()
              
              # Create database and warehouse
              print("🏗️ Creating infrastructure...")
              cursor.execute(f"CREATE DATABASE IF NOT EXISTS {database}")
              cursor.execute(f"CREATE WAREHOUSE IF NOT EXISTS {warehouse} WITH WAREHOUSE_SIZE='SMALL'")
              
              # Set context
              cursor.execute(f"USE DATABASE {database}")
              cursor.execute(f"USE WAREHOUSE {warehouse}")
              
              # Initialize variables for incremental deployment  
              changed_files = []
              
              # Load deployment configuration
              import yaml
              
              try:
                  with open('.github/deployment-config.yml', 'r') as f:
                      deployment_config = yaml.safe_load(f)
                  print(f"📋 Loaded deployment configuration with {len(deployment_config['deployment']['schemas'])} schemas")
              except Exception as e:
                  print(f"⚠️ Could not load deployment config: {e}")
                  print(f"📋 Using fallback configuration")
                  deployment_config = {
                      'deployment': {
                          'schemas': [
                              {'name': 'monitoring', 'dependencies': []},
                              {'name': 'raw_data', 'dependencies': ['monitoring']},
                              {'name': 'processed_data', 'dependencies': ['monitoring', 'raw_data']},
                              {'name': 'reporting', 'dependencies': ['monitoring', 'raw_data', 'processed_data']}
                          ]
                      }
                  }
              
              # Extract schema names in dependency order
              all_schema_names = [schema['name'] for schema in deployment_config['deployment']['schemas']]
              print(f"📋 Available schemas: {all_schema_names}")
              
              # Determine schemas to deploy
              target_schemas_input = '${{ needs.setup.outputs.target_schemas }}'.strip()
              
              if target_schemas_input:
                  # Use specified schemas (works with any deployment mode)
                  requested_schemas = [s.strip().lower() for s in target_schemas_input.split(',')]
                  print(f"🎯 Target schemas specified: {requested_schemas}")
                  
                  # Add dependencies automatically
                  schemas = []
                  schema_deps = {schema['name']: schema.get('dependencies', []) for schema in deployment_config['deployment']['schemas']}
                  
                  # Add required dependencies for requested schemas
                  for requested in requested_schemas:
                      if requested in schema_deps:
                          for dep in schema_deps[requested]:
                              if dep not in schemas and dep not in requested_schemas:
                                  schemas.append(dep)
                                  print(f"📎 Adding {dep} schema (required dependency for {requested})")
                  
                  # Add requested schemas in dependency order
                  for schema_name in all_schema_names:
                      if schema_name in requested_schemas and schema_name not in schemas:
                          schemas.append(schema_name)
                  
                  print(f"📋 Final schema list with dependencies: {schemas}")
              elif deployment_mode == "FULL":
                  schemas = all_schema_names
                  print(f"📋 Full deployment - all schemas: {schemas}")
              else:
                  # INCREMENTAL: detect changed files from git diff
                  import subprocess
                  import os
                  
                  try:
                      # Get list of changed SQL files since last deployment
                      # Use HEAD~1 to compare with previous commit, or origin/main for PR
                      if os.environ.get('GITHUB_EVENT_NAME') == 'pull_request':
                          base_ref = 'origin/main'
                      else:
                          base_ref = 'HEAD~1'
                      
                      print(f"🔍 Git diff command: git diff --name-only {base_ref} HEAD -- sql/")
                      
                      # First, let's check git status and recent commits
                      status_result = subprocess.run(['git', 'log', '--oneline', '-5'], 
                                                   capture_output=True, text=True, check=False)
                      print(f"📋 Recent commits:\\n{status_result.stdout}")
                      
                      result = subprocess.run(['git', 'diff', '--name-only', base_ref, 'HEAD', '--', 'sql/'], 
                                            capture_output=True, text=True, check=True)
                      changed_files = result.stdout.strip().split('\n') if result.stdout.strip() else []
                      
                      print(f"🔍 Changed SQL files: {changed_files}")
                      
                      # Determine which schemas need deployment based on changed files
                      schemas_to_deploy = set()
                      
                      for file_path in changed_files:
                          if file_path:
                              print(f"  📁 Analyzing: {file_path}")
                              
                              # Foundation files affect all schemas
                              if 'schemas/00_database_and_warehouse.sql' in file_path:
                                  schemas_to_deploy.update(all_schema_names)
                                  print(f"    🏗️ Foundation change - deploying all schemas")
                              
                              # Schema-specific changes - dynamically detect from config
                              else:
                                  schema_detected = False
                                  for schema_config in deployment_config['deployment']['schemas']:
                                      schema_name = schema_config['name']
                                      if f'schemas/{schema_name}/' in file_path:
                                          schemas_to_deploy.add(schema_name)
                                          
                                          # Add dependent schemas if configured
                                          dependent_schemas = [s['name'] for s in deployment_config['deployment']['schemas'] 
                                                             if schema_name in s.get('dependencies', [])]
                                          if dependent_schemas:
                                              schemas_to_deploy.update(dependent_schemas)
                                              print(f"    📥 {schema_name} schema change detected (also deploying: {', '.join(dependent_schemas)})")
                                          else:
                                              print(f"    📊 {schema_name} schema change detected")
                                          schema_detected = True
                                          break
                                  
                                  if not schema_detected:
                                      # Handle special cases
                                      if 'schemas/permissions.sql' in file_path:
                                          # Permissions affect all schemas
                                          schemas_to_deploy.update(all_schema_names)
                                          print(f"    🔐 Permissions change - deploying all schemas")
                                      elif file_path.startswith('sql/') and file_path.endswith('.sql'):
                                          # Other SQL files at root level
                                          schemas_to_deploy.update(all_schema_names)
                                          print(f"    🗄️ Root SQL change - deploying all schemas")
                      
                      # Convert to ordered list maintaining dependency order
                      schemas = [s for s in all_schema_names if s in schemas_to_deploy]
                      
                      if not schemas:
                          print(f"🔍 No SQL schema changes detected - skipping deployment")
                          schemas = []
                      else:
                          print(f"📋 Incremental deployment - changed schemas: {schemas}")
                          
                  except subprocess.CalledProcessError as e:
                      print(f"⚠️ Git diff failed: {e}")
                      print(f"⚠️ Git diff stderr: {e.stderr}")
                      print(f"⚠️ Git diff stdout: {e.stdout}")
                      print(f"📋 Fallback: deploying all schemas")
                      schemas = all_schema_names
                  except Exception as e:
                      print(f"⚠️ Error detecting changes: {e}")
                      print(f"📋 Fallback: deploying all schemas")
                      schemas = all_schema_names
              
              print(f"📋 Deploying schemas: {', '.join(schemas)}")
              
              # Check if there are any schemas to deploy
              if not schemas:
                  print("🎯 No schemas to deploy - exiting early")
                  exit(0)
              
              # Deploy schemas following the deployment order
              deployment_files = []
              
              # Include foundation setup only if deploying schemas
              deployment_files.append("schemas/00_database_and_warehouse.sql")
              
              # Dynamically discover schema files from repository structure using config
              import glob
              schema_files = {}
              
              # Get file patterns from config
              include_patterns = deployment_config.get('file_patterns', {}).get('include', ['*.sql'])
              exclude_patterns = deployment_config.get('file_patterns', {}).get('exclude', [])
              
              for schema_config in deployment_config['deployment']['schemas']:
                  schema_name = schema_config['name']
                  schema_path = f"sql/schemas/{schema_name}/"
                  
                  if os.path.exists(schema_path):
                      # Find SQL files matching include patterns
                      sql_files = []
                      for pattern in include_patterns:
                          sql_files.extend(glob.glob(f"{schema_path}{pattern}"))
                      
                      # Filter out excluded files
                      for exclude_pattern in exclude_patterns:
                          excluded_files = glob.glob(f"{schema_path}{exclude_pattern}")
                          sql_files = [f for f in sql_files if f not in excluded_files]
                      
                      if sql_files:
                          # Convert to relative paths from sql/ directory
                          relative_files = [f"schemas/{schema_name}/{os.path.basename(f)}" for f in sql_files]
                          # Sort files for consistent deployment order
                          relative_files.sort()
                          schema_files[schema_name] = relative_files
                          print(f"📁 Discovered {len(relative_files)} files for {schema_name} schema: {relative_files}")
                      else:
                          print(f"⚠️ No SQL files found in {schema_path} matching patterns {include_patterns}")
                  else:
                      print(f"⚠️ Schema directory not found: {schema_path}")
              
              print(f"📋 Total discovered schema files: {sum(len(files) for files in schema_files.values())}")
              
              # Add files for target schemas in dependency order - but only changed files in incremental mode
              if deployment_mode == "INCREMENTAL":
                  # For incremental, only add files that have actually changed
                  for schema in schemas:
                      if schema in schema_files:
                          schema_changed_files = []
                          for file_path in schema_files[schema]:
                              full_file_path = f"sql/{file_path}"
                              if full_file_path in changed_files:
                                  schema_changed_files.append(file_path)
                                  print(f"📁 Will deploy changed file: {file_path}")
                          
                          if schema_changed_files:
                              deployment_files.extend(schema_changed_files)
                              print(f"📁 Added {len(schema_changed_files)} changed files for {schema} schema")
                          else:
                              print(f"📁 No changed files detected for {schema} schema")
              else:
                  # For FULL and SCHEMA_SPECIFIC modes, deploy all files for target schemas
                  for schema in schemas:
                      if schema in schema_files:
                          deployment_files.extend(schema_files[schema])
                          print(f"📁 Added {len(schema_files[schema])} files for {schema} schema")
              
              # Always include permissions at the end if any schema is deployed
              if schemas:
                  deployment_files.append("schemas/permissions.sql")
              
              print(f"📋 Deploying {len(deployment_files)} SQL files in order...")
              
              for sql_file in deployment_files:
                  file_path = f"sql/{sql_file}"
                  print(f"🗄️ Executing: {file_path}")
                  
                  try:
                      # Execute SQL file using execute_stream for full file execution
                      print(f"  📝 Executing SQL file using execute_stream")
                      
                      try:
                          # Open file and use execute_stream to execute entire file
                          with open(file_path, 'r') as f:
                              # Replace hardcoded database name with environment-specific database
                              sql_content = f.read()
                              sql_content = sql_content.replace('USE DATABASE analytics_platform;', f'USE DATABASE {database};')
                              sql_content = sql_content.replace('CREATE OR ALTER DATABASE analytics_platform;', f'CREATE OR ALTER DATABASE {database};')
                              print(f"  🔄 Database name replaced: analytics_platform -> {database}")
                              
                              # Create a temporary file-like object from the modified content
                              from io import StringIO
                              sql_stream = StringIO(sql_content)
                              
                              # Execute the entire file content as a stream
                              results = conn.execute_stream(sql_stream)
                              
                              # Process results
                              statement_count = 0
                              for result in results:
                                  statement_count += 1
                                  try:
                                      # Get result details if available
                                      if hasattr(result, 'fetchall'):
                                          rows = result.fetchall()
                                          print(f"  ✅ Statement {statement_count} executed successfully ({len(rows)} rows affected)")
                                      else:
                                          print(f"  ✅ Statement {statement_count} executed successfully")
                                  except Exception as stmt_error:
                                      error_msg = str(stmt_error).lower()
                                      # Treat certain errors as warnings, others as failures
                                      if any(warning in error_msg for warning in ['already exists', 'does not exist', 'if exists']):
                                          print(f"  ⚠️ Statement {statement_count} warning: {stmt_error}")
                                      else:
                                          print(f"  ❌ Statement {statement_count} failed: {stmt_error}")
                                          if deployment_mode == "SCHEMA_SPECIFIC":
                                              raise stmt_error
                                          # Continue with other statements for other modes
                              
                              print(f"  ✅ SQL file executed successfully ({statement_count} statements)")
                              
                      except Exception as file_error:
                          print(f"  ❌ File execution failed: {file_error}")
                          if deployment_mode == "SCHEMA_SPECIFIC":
                              raise file_error
                      
                      print(f"✅ File {sql_file} completed")
                      
                  except FileNotFoundError:
                      print(f"⚠️ File not found: {file_path} - skipping")
                  except Exception as file_error:
                      print(f"❌ Error processing {file_path}: {file_error}")
                      # Exit on individual file errors for FULL and SCHEMA_SPECIFIC modes
                      if deployment_mode in ["FULL", "SCHEMA_SPECIFIC"]:
                          raise file_error
              
              # Create deployment tracking in monitoring schema
              try:
                  cursor.execute("""
                      CREATE OR REPLACE TABLE monitoring.github_deployments (
                          run_id VARCHAR(50),
                          deployment_time TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
                          environment VARCHAR(50),
                          git_sha VARCHAR(50),
                          deployment_mode VARCHAR(50),
                          actor VARCHAR(100),
                          files_deployed NUMBER,
                          status VARCHAR(20)
                      )
                  """)
                  
                  cursor.execute(f"""
                      INSERT INTO monitoring.github_deployments VALUES (
                          '${{ github.run_id }}',
                          CURRENT_TIMESTAMP(),
                          '${{ needs.setup.outputs.target_environment }}',
                          '${{ github.sha }}',
                          '{deployment_mode}',
                          '${{ github.actor }}',
                          {len(deployment_files)},
                          'SUCCESS'
                      )
                  """)
                  
                  print("✅ Deployment tracking record created")
              except Exception as track_error:
                  print(f"⚠️ Could not create deployment tracking: {track_error}")
              
              cursor.close()
              conn.close()
              print("🎉 Deployment completed successfully!")
              
              # Write success log
              with open('deployment-logs/deployment.log', 'w') as f:
                  f.write(f"Deployment to ${{ needs.setup.outputs.target_environment }} completed successfully at {datetime.now()}\n")
                  f.write(f"Database: {database}\n")
                  f.write(f"Warehouse: {warehouse}\n")
                  f.write(f"Files deployed: {len(deployment_files)}\n")
                  f.write(f"Deployment mode: {deployment_mode}\n")
                  for sql_file in deployment_files:
                      f.write(f"  - {sql_file}\n")
              
          except Exception as e:
              print(f"❌ Deployment failed: {e}")
              with open('deployment-logs/deployment.log', 'w') as f:
                  f.write(f"Deployment to ${{ needs.setup.outputs.target_environment }} failed at {datetime.now()}\n")
                  f.write(f"Error: {str(e)}\n")
              exit(1)
          EOF
          
      - name: 🔍 Dry Run Report
        if: needs.setup.outputs.dry_run == 'true'
        run: |
          echo "🔍 DRY RUN - No actual deployment performed"
          echo ""
          echo "## 🗄️ Deployment Plan" >> $GITHUB_STEP_SUMMARY
          echo "**Environment:** ${{ needs.setup.outputs.target_environment }}" >> $GITHUB_STEP_SUMMARY
          echo "**Database:** ${{ needs.setup.outputs.snowflake_database }}" >> $GITHUB_STEP_SUMMARY
          echo "**Warehouse:** ${{ needs.setup.outputs.snowflake_warehouse }}" >> $GITHUB_STEP_SUMMARY
          echo "**Mode:** ${{ needs.setup.outputs.deployment_mode }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Would deploy SQL files:" >> $GITHUB_STEP_SUMMARY
          echo "- Database: \`${{ needs.setup.outputs.snowflake_database }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Warehouse: \`${{ needs.setup.outputs.snowflake_warehouse }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Foundation: 00_database_and_warehouse.sql" >> $GITHUB_STEP_SUMMARY
          echo "- Monitoring: tables.sql, stored_procedures.sql" >> $GITHUB_STEP_SUMMARY
          echo "- Raw Data: tables.sql, procedures, streams, tasks" >> $GITHUB_STEP_SUMMARY
          echo "- Processed Data: tables.sql, procedures" >> $GITHUB_STEP_SUMMARY
          echo "- Reporting: views.sql" >> $GITHUB_STEP_SUMMARY
          echo "- Permissions: permissions.sql" >> $GITHUB_STEP_SUMMARY
          echo "- Total: 18 SQL files with tables, views, procedures, and tasks" >> $GITHUB_STEP_SUMMARY
          
      - name: 🔍 Post-Deployment Verification
        if: needs.setup.outputs.dry_run == 'false'
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets[format('SNOWFLAKE_{0}_ACCOUNT', needs.setup.outputs.target_environment)] }}
          SNOWFLAKE_USER: ${{ secrets[format('SNOWFLAKE_{0}_USER', needs.setup.outputs.target_environment)] }}
          SNOWFLAKE_PASSWORD: ${{ secrets[format('SNOWFLAKE_{0}_PASSWORD', needs.setup.outputs.target_environment)] }}
        run: |
          echo "🔍 Verifying deployment..."
          
          python3 << 'EOF'
          import snowflake.connector
          import os
          
          try:
              conn = snowflake.connector.connect(
                  account=os.environ['SNOWFLAKE_ACCOUNT'],
                  user=os.environ['SNOWFLAKE_USER'],
                  password=os.environ['SNOWFLAKE_PASSWORD'],
                  warehouse='${{ needs.setup.outputs.snowflake_warehouse }}',
                  database='${{ needs.setup.outputs.snowflake_database }}'
              )
              cursor = conn.cursor()
              
              # Verify database
              print("📋 Checking database...")
              cursor.execute("SELECT CURRENT_DATABASE()")
              db_result = cursor.fetchone()
              print(f"✅ Connected to database: {db_result[0]}")
              
              # Verify warehouse  
              print("🏭 Checking warehouse...")
              cursor.execute("SELECT CURRENT_WAREHOUSE()")
              wh_result = cursor.fetchone()
              print(f"✅ Using warehouse: {wh_result[0]}")
              
              # Verify schemas
              print("🗄️ Checking schemas...")
              cursor.execute("SHOW SCHEMAS")
              schemas = cursor.fetchall()
              schema_names = [schema[1] for schema in schemas]  # Schema name is in second column
              print(f"✅ Found schemas: {', '.join(schema_names)}")
              
              # Check deployed objects
              print("📊 Verifying deployed objects...")
              
              # Check tables in each schema
              verification_checks = [
                  ("monitoring", "TABLES", "job_executions"),
                  ("raw_data", "TABLES", "orders"),
                  ("processed_data", "TABLES", "customer_metrics"),
                  ("reporting", "VIEWS", "sales_summary")
              ]
              
              for schema, object_type, sample_object in verification_checks:
                  if schema.upper() in [s.upper() for s in schema_names]:
                      try:
                          cursor.execute(f"SHOW {object_type} IN SCHEMA {schema}")
                          objects = cursor.fetchall()
                          object_count = len(objects)
                          print(f"✅ Found {object_count} {object_type.lower()} in {schema} schema")
                          
                          # Check for specific sample object
                          object_names = [obj[1] for obj in objects]  # Object name is in second column
                          if sample_object.upper() in [obj.upper() for obj in object_names]:
                              print(f"  ✅ Verified {sample_object} exists")
                          else:
                              print(f"  ⚠️ Sample object {sample_object} not found")
                              
                      except Exception as e:
                          print(f"⚠️ Could not verify {object_type.lower()} in {schema}: {e}")
                  else:
                      print(f"⚠️ Schema {schema} not found")
              
              # Check deployment tracking
              try:
                  cursor.execute("SELECT COUNT(*) FROM monitoring.github_deployments WHERE run_id = '${{ github.run_id }}'")
                  count = cursor.fetchone()[0]
                  if count > 0:
                      print(f"✅ Found deployment tracking record")
                  else:
                      print(f"⚠️ No deployment tracking record found")
              except Exception as e:
                  print(f"⚠️ Could not check deployment tracking: {e}")
              
              cursor.close()
              conn.close()
              print("✅ Verification completed")
              
          except Exception as e:
              print(f"❌ Verification failed: {e}")
              exit(1)
          EOF
          
      - name: 📊 Upload Deployment Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: deployment-logs-${{ needs.setup.outputs.target_environment }}
          path: deployment-logs/
          
      - name: 📈 Generate Deployment Summary
        if: needs.setup.outputs.dry_run == 'false'
        run: |
          echo "## 🎉 Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Environment | ${{ needs.setup.outputs.target_environment }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Database | ${{ needs.setup.outputs.snowflake_database }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Warehouse | ${{ needs.setup.outputs.snowflake_warehouse }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Run ID | ${{ github.run_id }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Git SHA | ${{ github.sha }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Actor | ${{ github.actor }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ **Deployment completed successfully!**" >> $GITHUB_STEP_SUMMARY

  # Job 5: Load Sample Data (Optional)
  load_sample_data:
    name: 📊 Load Sample Data
    runs-on: ubuntu-latest
    needs: [setup, deploy]
    if: always() && needs.setup.outputs.dry_run == 'false' && needs.setup.outputs.load_sample_data == 'true' && needs.deploy.result == 'success'
    
    steps:
      - name: 🔍 Debug Sample Data Conditions
        run: |
          echo "🔍 Checking sample data loading conditions:"
          echo "  - Dry run: '${{ needs.setup.outputs.dry_run }}' (should be 'false')"
          echo "  - Load sample data: '${{ needs.setup.outputs.load_sample_data }}' (should be 'true')" 
          echo "  - Deploy result: '${{ needs.deploy.result }}' (should be 'success')"
          echo "  - Condition result: ${{ needs.setup.outputs.dry_run == 'false' && needs.setup.outputs.load_sample_data == 'true' && needs.deploy.result == 'success' }}"
          
      - name: 📥 Checkout Code
        uses: actions/checkout@v4
        
      - name: 🔧 Install Dependencies
        run: |
          echo "📦 Installing Python dependencies..."
          pip install snowflake-connector-python
          
      - name: 📊 Load Sample Data
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets[format('SNOWFLAKE_{0}_ACCOUNT', needs.setup.outputs.target_environment)] }}
          SNOWFLAKE_USER: ${{ secrets[format('SNOWFLAKE_{0}_USER', needs.setup.outputs.target_environment)] }}
          SNOWFLAKE_PASSWORD: ${{ secrets[format('SNOWFLAKE_{0}_PASSWORD', needs.setup.outputs.target_environment)] }}
        run: |
          echo "📊 Loading sample data to ${{ needs.setup.outputs.target_environment }}..."
          
          python3 << 'EOF'
          import snowflake.connector
          import os
          
          # Connection parameters
          conn_params = {
              'account': os.environ['SNOWFLAKE_ACCOUNT'],
              'user': os.environ['SNOWFLAKE_USER'],
              'password': os.environ['SNOWFLAKE_PASSWORD'],
              'warehouse': '${{ needs.setup.outputs.snowflake_warehouse }}',
              'database': '${{ needs.setup.outputs.snowflake_database }}'
          }
          
          try:
              print("🔗 Connecting to Snowflake...")
              conn = snowflake.connector.connect(**conn_params)
              cursor = conn.cursor()
              
              # Read and execute sample data file
              print("📊 Loading sample data from 02_sample_data.sql...")
              with open('sql/02_sample_data.sql', 'r') as f:
                  sql_content = f.read()
              
              # Replace hardcoded database name with environment-specific database
              database = '${{ needs.setup.outputs.snowflake_database }}'
              sql_content = sql_content.replace('USE DATABASE analytics_platform;', f'USE DATABASE {database};')
              
              # Execute sample data by splitting on semicolons (no procedures in sample data)
              statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip()]
              print(f"📊 Executing {len(statements)} sample data statements...")
              
              for i, statement in enumerate(statements, 1):
                  if statement:
                      try:
                          cursor.execute(statement)
                          print(f"  ✅ Statement {i} executed successfully")
                      except Exception as stmt_error:
                          print(f"  ⚠️ Statement {i} warning: {stmt_error}")
                          # Continue with sample data loading even on errors
              
              cursor.close()
              conn.close()
              print("🎉 Sample data loaded successfully!")
              
          except Exception as e:
              print(f"❌ Sample data loading failed: {e}")
              exit(1)
          EOF

  # Job 6: Notification
  notify:
    name: 📢 Notification
    runs-on: ubuntu-latest
    needs: [setup, deploy, approval, load_sample_data]
    if: always()
    
    steps:
      - name: 📢 Deployment Notification
        run: |
          echo "🔍 Debug Information:"
          echo "  - Deploy result: ${{ needs.deploy.result }}"
          echo "  - Load sample data result: ${{ needs.load_sample_data.result }}"
          echo "  - Dry run setting: ${{ needs.setup.outputs.dry_run }}"
          echo "  - Load sample data setting: ${{ needs.setup.outputs.load_sample_data }}"
          echo ""
          
          if [ "${{ needs.deploy.result }}" = "success" ]; then
            echo "🎉 Deployment to ${{ needs.setup.outputs.target_environment }} completed successfully!"
          elif [ "${{ needs.deploy.result }}" = "failure" ]; then
            echo "❌ Deployment to ${{ needs.setup.outputs.target_environment }} failed!"
          elif [ "${{ needs.deploy.result }}" = "skipped" ]; then
            echo "⏭️ Deployment to ${{ needs.setup.outputs.target_environment }} was skipped (dry run or no approval)"
          fi
          
          if [ "${{ needs.load_sample_data.result }}" = "success" ]; then
            echo "📊 Sample data loaded successfully!"
          elif [ "${{ needs.load_sample_data.result }}" = "skipped" ]; then
            echo "⏭️ Sample data loading was skipped"
          elif [ "${{ needs.load_sample_data.result }}" = "failure" ]; then
            echo "❌ Sample data loading failed!"
          fi